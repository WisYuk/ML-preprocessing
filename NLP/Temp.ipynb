{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.10/site-packages (2.3.0+cpu)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, RobertaTokenizer, TFRobertaForSequenceClassification,pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 18:21:02.682963: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "2024-06-09 18:21:04.031334: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "2024-06-09 18:21:04.065532: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "2024-06-09 18:21:10.082735: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = TFBertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "tokenizer1 = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model1 = TFRobertaForSequenceClassification.from_pretrained('roberta-large')\n",
    "\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True)\n",
    "    # Get model outputs\n",
    "    outputs = model(**inputs)\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "    # Convert probabilities to numpy array and get the sentiment scores\n",
    "    scores = probs.numpy()[0]\n",
    "    # Determine the sentiment\n",
    "    sentiment = np.argmax(scores)\n",
    "    return sentiment, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment1(text):\n",
    "    # Tokenisasi input text\n",
    "    inputs = tokenizer1(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Melakukan prediksi menggunakan model\n",
    "    outputs = model1(inputs)\n",
    "    \n",
    "    # Mengambil logits dari output\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Debug: print logits\n",
    "    print(f\"Logits: {logits}\")\n",
    "    \n",
    "    # Menggunakan softmax untuk mendapatkan probabilitas\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Debug: print probabilities\n",
    "    print(f\"Probabilities: {probabilities}\")\n",
    "    \n",
    "    # Mendapatkan label sentimen\n",
    "    predicted_class = tf.argmax(probabilities, axis=-1).numpy()[0]\n",
    "    \n",
    "    # Debug: print predicted_class\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    \n",
    "    # Mengembalikan hasil sentimen\n",
    "    sentiment = \"Positif\" if predicted_class == 1 else \"Negatif\"\n",
    "    return sentiment, probabilities.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anlsis_sentimen(text):\n",
    "    result = sentiment_analyzer(text)\n",
    "    sentiment = result[0]['label']\n",
    "    score = result[0]['score']\n",
    "    return sentiment, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "to_translate = 'senang'\n",
    "translated = GoogleTranslator(source='id', target='en').translate(to_translate)\n",
    "print(translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am very happy with this service.\n",
      "Text: Saya sangat senang dengan layanan ini.\n",
      "Sentiment: 4\n",
      "Sentiment_label: Positive\n",
      "Scores: [0.00211245 0.00171952 0.01492568 0.22245784 0.75878453]\n",
      "\n",
      "The service is very bad and disappointing.\n",
      "Text: Pelayanannya sangat buruk dan mengecewakan.\n",
      "Sentiment: 0\n",
      "Sentiment_label: Negative\n",
      "Scores: [6.0907114e-01 3.5716170e-01 3.2291558e-02 1.1160544e-03 3.5953449e-04]\n",
      "\n",
      "The food is very delicious, I like it!\n",
      "Text: Makanannya enak sekali, saya suka!\n",
      "Sentiment: 4\n",
      "Sentiment_label: Positive\n",
      "Scores: [0.00146615 0.00214573 0.03250634 0.43027803 0.5336037 ]\n",
      "\n",
      "The price is too expensive for this quality.\n",
      "Text: Harganya terlalu mahal untuk kualitas seperti ini.\n",
      "Sentiment: 1\n",
      "Sentiment_label: Negative\n",
      "Scores: [0.20227702 0.51076794 0.2629789  0.01998584 0.00399031]\n",
      "\n",
      "having a good time\n",
      "Text: senang senang\n",
      "Sentiment: 3\n",
      "Sentiment_label: Positive\n",
      "Scores: [0.02286    0.03521525 0.1855145  0.45422596 0.30218422]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample texts for sentiment analysis\n",
    "sample_texts = [\n",
    "    \"Saya sangat senang dengan layanan ini.\",\n",
    "    \"Pelayanannya sangat buruk dan mengecewakan.\",\n",
    "    \"Makanannya enak sekali, saya suka!\",\n",
    "    \"Harganya terlalu mahal untuk kualitas seperti ini.\",\n",
    "    \"senang senang\"\n",
    "]\n",
    "    \n",
    "# Analyze sentiment for each sample text\n",
    "for text in sample_texts:\n",
    "    to_translate = text\n",
    "    translated = GoogleTranslator(source='id', target='en').translate(to_translate)\n",
    "    print(translated)\n",
    "    sentiment, scores = analyze_sentiment(translated)\n",
    "    sentiment_label = 'Positive' if sentiment > 2 else 'Negative'\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(f\"Sentiment_label: {sentiment_label}\")\n",
    "    print(f\"Scores: {scores}\\n\")\n",
    "\n",
    "for text in sample_texts:\n",
    "    to_translate = text\n",
    "    translated = GoogleTranslator(source='id', target='en').translate(to_translate)\n",
    "    print(translated)\n",
    "    # sentiment, scores = analyze_sentiment1(translated)\n",
    "    # hasil = analyze_sentiment1(translated)\n",
    "    # print(hasil)\n",
    "    sentimen, probabilitas = analyze_sentiment1(translated)\n",
    "    print(f\"Sentimen: {sentimen}, Probabilitas: {probabilitas}\")\n",
    "\n",
    "    # sentiment_label = 'Positive' if sentiment > 2 else 'Negative'\n",
    "    # print(f\"Text: {text}\")\n",
    "    # print(f\"Sentiment: {sentiment}\")\n",
    "    # print(f\"Sentiment_label: {sentiment_label}\")\n",
    "    # print(f\"Scores: {scores}\\n\")\n",
    "\n",
    "for text in sample_texts:\n",
    "    to_translate = text\n",
    "    translated = GoogleTranslator(source='id', target='en').translate(to_translate)\n",
    "    print(translated)\n",
    "    # sentiment, scores = analyze_sentiment1(translated)\n",
    "    # hasil = analyze_sentiment1(translated)\n",
    "    # print(hasil)\n",
    "    # sentimen, probabilitas = analyze_sentiment1(text)\n",
    "    # print(f\"Sentimen: {sentimen}, Probabilitas: {probabilitas}\")\n",
    "\n",
    "    # sentiment_label = 'Positive' if sentiment > 2 else 'Negative'\n",
    "    # print(f\"Text: {text}\")\n",
    "    # print(f\"Sentiment: {sentiment}\")\n",
    "    # print(f\"Sentiment_label: {sentiment_label}\")\n",
    "    # print(f\"Scores: {scores}\\n\")\n",
    "    sentimen, skor = anlsis_sentimen(translated)\n",
    "    print(f\"Teks: {text}\\nSentimen: {sentimen}, Skor: {skor}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Function to clean the specific noise from the text\n",
    "def clean_text(text):\n",
    "    # Remove remains of emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove remains of user mentions\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    # Remove Twitter links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove retweet identifiers\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    text = re.sub(r'\\[RE.*?\\]', '', text)\n",
    "    # Remove HTML entities like &amp;\n",
    "    text = re.sub(r'&\\S+;', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv('devina.csv')\n",
    "\n",
    "\n",
    "# drop row \n",
    "df = df.drop('Place Name', axis=1)\n",
    "df = df.drop('quote_count', axis=1)\n",
    "df = df.drop('favorite_count', axis=1)\n",
    "df = df.drop('reply_count', axis=1)\n",
    "df = df.drop('retweet_count', axis=1)\n",
    "\n",
    "# Convert 'created_at' to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "# Extract month and create a new column 'month'\n",
    "df['month'] = df['created_at'].dt.month\n",
    "\n",
    "df = df.drop('created_at', axis=1)\n",
    "# df2 = df.drop('favorite_count', axis=1)\n",
    "# Sort DataFrame by 'id' and then 'month'\n",
    "df = df.sort_values(['id', 'month'])\n",
    "\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_text)\n",
    "# add new row called score and fill with random value betwen 0-1\n",
    "df['score'] = np.random.rand(len(df))\n",
    "\n",
    "df = df.drop('full_text', axis=1)\n",
    "\n",
    "\n",
    "## penghujan\n",
    "\n",
    "# Filter DataFrame for months 1 to 6\n",
    "df_filtered = df[(df['month'] >= 1) & (df['month'] <= 6)]\n",
    "\n",
    "# Group by 'id' and calculate average 'score'\n",
    "average_scores = df_filtered.groupby('id')['score'].mean()\n",
    "\n",
    "# Convert Series to dictionary\n",
    "average_scores_map = average_scores.to_dict()\n",
    "\n",
    "# Rename the dictionary to 'month_kemarau'\n",
    "month_penghujan = average_scores_map\n",
    "\n",
    "#print(month_penghujan)\n",
    "\n",
    "# Convert 'month_penghujan' to DataFrame\n",
    "df_month_penghujan = pd.DataFrame.from_dict(month_penghujan, orient='index', columns=['average_score'])\n",
    "\n",
    "\n",
    "\n",
    "## kemarau\n",
    "\n",
    "# Filter DataFrame for months 1 to 6\n",
    "df_filtered = df[(df['month'] >= 7) & (df['month'] <= 12)]\n",
    "\n",
    "# Group by 'id' and calculate average 'score'\n",
    "average_scores = df_filtered.groupby('id')['score'].mean()\n",
    "\n",
    "# Convert Series to dictionary\n",
    "average_scores_map = average_scores.to_dict()\n",
    "\n",
    "# Rename the dictionary to 'month_kemarau'\n",
    "month_kemarau = average_scores_map\n",
    "\n",
    "\n",
    "# Convert 'month_penghujan' to DataFrame\n",
    "df_month_kemarau = pd.DataFrame.from_dict(month_kemarau, orient='index', columns=['average_score'])\n",
    "\n",
    "\n",
    "print(df_month_penghujan)\n",
    "print(df_month_kemarau)\n",
    "#print(df.head())\n",
    "\n",
    "# convert to csv\n",
    "df.to_csv('devina_clean.csv', index=False)\n",
    "df_month_penghujan.to_csv('devina_penghujan.csv', index=True)\n",
    "df_month_kemarau.to_csv('devina_kemarau.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
